# -*- coding: utf-8 -*-
"""Extraccion Submissions de Reddit usando Pushshift.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kOOw449cWgjhTQYUvUVNTuKXMbMQF4vo

# Descarga de submissions de Reddit mediante el modulo Pushshift usando Python

Para descarga de información en Reddit contamos con muchas herramientas como PRAW, PMAW y requests.

PMAW es derivado de PRAW y ambos tienen algunas limitantes, como la eliminación del   *subreddit.submissions* que nos permite seleccionar la información que será descargada.

Es por esto que se opta por utilizar  *requests* para acceder a Reddit y obtener la información almacenada en **Pushshift**.

# Importar módulos
"""

import pandas as pd # Uso de dataframes
import requests # Método para acceder a Pushshift por mediante url 
import json # Manipulación de información JSON
import csv # Para convertir las tablas finales en archivos csv y guardarla en la máquina local
import time # Convertir el tiempo UTC en tiempo GMT
import datetime # Manipulación del tiempo
#from google.colab import drive
#drive.mount('/content/drive')
#location = "/content/drive/MyDrive/Pushshift/"

"""# Ejemplos de URLS de Pushshift"""

# Se puede acceder al API de Pushshift a través de una URL con los parámetros relevantes sin necesidad de identificarse en Reddit
# Estos son algunos ejemplos de URLs que generan una página con información JSON
busca_gaming_despues_fecha = "https://api.pushshift.io/reddit/search/submission/?q=screenshot&after=1514764800&before=1517443200&subreddit=gaming"
busca_science = "https://api.pushshift.io/reddit/search/submission/?q=science"

"""# Parámetros para el URL de Pushshift
Estos son los parámetros que se conideran más importantes para la consulta de Reddit, utilizados para la construcción del URL de Pushshift:

* size — Tamaño de entradas devueltas, modificado a 1,000
* after — Dónde comienza la búsqueda 
* before — Dónde finaliza la búsqueda
* title — Busca submissions que contengan la palabra en el título
* subreddit — Busca en un Subreddit en particular

Todo esto con la finalidad de reducir el tamaño de la búsqueda.
"""

# Adaptado de https://gist.github.com/dylankilkenny/3dbf6123527260165f8c5c3bc3ee331b
# Se construye la URL de Pushshift, accediendo a una pagina web donde está almacenada la información en JSON a manera de lista
def getPushshiftData(query, after, before, sub):
    # Construye la URL de Pushshift con los parámetros antes mencionados
    url = 'https://api.pushshift.io/reddit/search/submission/?title='+str(query)+'&size=1000&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)
    # Muestra al usuario la URL 
    print(url)
    # Solicitud de la URL
    r = requests.get(url, headers = {'User-agent': 'bot_1'})
    r.raise_for_status()
    # Carga la pagina con la información JSON en una variable
    if r.status_code != 204:
      data = json.loads(r.text, strict=False,encoding='utf-8')
    #regresa la información de la variable donde está contenida
    return data['data']

"""# Extrae la información clave de Submissions

Es aquí donde tenemos la libertad de solicitar la información que requerimos de Submissions para nuestro análisis: 
* Title: Título de la noticia
* URL: URL de la noticia
* Subreddit: Grupo en el que se publicó el submission
* Selftext: Contenido de la noticia, no todas las noticiac cuentan con esto
* Flair: Tópico del submission
* Author: Usuario que publicó el submission
* ID: ID para identificar el submission
* Score: Puntuación del submission
* Created_utc: Fecha de publicación
* Num_comments: Número de comentarios 
* Permalink: Enlace permanente.

"""

# Extracción de la información específica de los datos JSON generados  
# Seleccionar los parámetros deseados de pushshift: https://pushshift.io/api-parameters/
def collectSubData(subm):
    #subData fue creado al inicio donde está toda la información para ser agregados a la variable global subStats.
    subData = list() # lista para almacenar la información
    title = subm['title'] # Titulo 
    url = subm['url'] # URL
    subreddit = subm['subreddit'] # Grupo Subreddit
    # selftext, o cuerpo del post, no siempre está presente en submissions, para evitar errores se usa try/except
    try:
      body = subm['selftext']
    except KeyError:
      body = ''
    #flairs no siempre está presente en submissions, para evitar errores se usa try/except
    try:
        flair = subm['link_flair_text']
    except KeyError:
        flair = "NaN"    
    author = subm['author'] # Autor del post
    sub_id = subm['id'] # ID del post
    score = subm['score'] # Puntuación del post
    created = datetime.datetime.fromtimestamp(subm['created_utc']) # Fecha de creación en UTC, por lo que se hace la conversión
    numComms = subm['num_comments'] # Número de comentarios
    permalink = subm['permalink'] # Linl permanente

    #Junta toda la información en una tubla y se agrega a subData
    subData.append((sub_id,title,url,subreddit,body,author,score,created,numComms,permalink,flair))
    # Crea un diccionario con la entrada de información del submission actual y almacena toda la información relacionada con el submission
    subStats[sub_id] = subData

"""# Actualiza los parámetros de búsqueda"""

# Para tener el formato correcto de tiempo de la URL se puede usar la siguiente página
#https://www.unixtimestamp.com/index.php > Esto para crear el propio timestamp
after = "1577836800" #Submissions despues del timestamp 01 Enero 2020 0:00:0 1577836800
before = "1640995200" #Submissions antes del timestamp 01 Enero 2022 0:00:0 1640995200
query = "" # Palabra clave para buscar en submissions
sub = "wallstreetbets" #Cual Subreddit se buscará la información

#subCount cuenta el no. del total de envíos que se recopila
subCount = 0
#subStats es el diccionario donde almacenaremos nuestros datos.
subStats = {}

# Necesitamos ejecutar esta función fuera del ciclo primero para obtener después la variable actualizada
data = getPushshiftData(query, after, before, sub)
# Se ejecutará hasta que se hayan recopilado todas las publicaciones, es decir, cuando la longitud de la variable de datos = 0
# desde la fecha 'posterior' hasta la fecha 'anterior'
while len(data) > 0: #La longitud de los datos es el número de submission(data[0],data[1], etc.), una vez que llega a cero (después y antes de que vars sean iguales) finaliza
    for submission in data:
        collectSubData(submission)
        subCount+=1
    # Llama getPushshiftData() Con la fecha creada del ultimo submission
    print(len(data))
    print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))
    #actualiza la variable 'after' a la última fecha creada del submission
    after = data[-1]['created_utc']
    # datos han cambiado debido a la nueva variable posterior proporcionada por el código anterior
    data = getPushshiftData(query, after, before, sub)
    
print(len(data))

"""# Revisión de que la extracción del submission se haya cumplido con exito"""

print(str(len(subStats)) + " submissions agregado a la lista")
print("1st entrada es:")
print(list(subStats.values())[0][0][1] + " creado por: " + str(list(subStats.values())[0][0][5]))
print("Ultima entrada es:")
print(list(subStats.values())[-1][0][1] + " creado por: " + str(list(subStats.values())[-1][0][5]))

"""# Guardado de información en archivo CSV"""

def updateSubs_file(location):
    upload_count = 0
    print("Ingresa el nombre del archivo agregando .csv al final ")
    #location = "/content/drive/MyDrive/Pushshift/Parts/"
    filename = input() #Nombre que puso al archivo csv
    file = location + filename
    with open(file, 'w', newline='', encoding='utf-8') as file: 
        a = csv.writer(file, delimiter=',')
        headers = ["Post ID","Title","Url","Subreddit","Body","Author","Score","Publish Date","Total No. of Comments","Permalink","Flair"]
        a.writerow(headers)
        for sub in subStats:
            a.writerow(subStats[sub][0])
            upload_count+=1
            
        print(str(upload_count) + " submissions han sido guardados en un archivo csv")

updateSubs_file(location+"Parts/")

# Une todos los archivos con terminación .csv en uno solo, esto debido a que se descargó por partes las noticias
import glob
example = []
for infile in glob.glob(location+"Parts/*.csv"):
  data = pd.read_csv(infile)
  example.append(data)
appended_data = pd.concat(example)
file_all = (location+"/WallStreetbets.csv")
appended_data.to_csv(file_all)

"""# Información de Yahoo Finance"""

#pip install yfinance

import yfinance as yf
tick="GME"
start='2020-01-01'
end='2022-01-01'

df_y=yf.download(tick,start, end, progress=False)
df_y.head(5)

file_Yaho = (location+"Finance/Yahoo_GME.csv")

df_y.to_csv(file_Yaho)

"""# Analisis de información

"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np 
import pandas as pd
import matplotlib
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import datetime as dt
# %matplotlib inline 
from wordcloud import WordCloud, STOPWORDS
from nltk.sentiment import SentimentIntensityAnalyzer
from textblob import TextBlob
from collections import Counter
import re
import string
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from nltk.sentiment.vader import SentimentIntensityAnalyzer as vad
from sklearn import preprocessing

data = pd.read_csv(location +"WallStreetbets.csv")
data_y = pd.read_csv(location +"Finance/Yahoo_GME.csv")
del data['Unnamed: 0']

# Commented out IPython magic to ensure Python compatibility.
# %pylab inline
 
pylab.rcParams['figure.figsize'] = (15, 9)   # Change the size of plots

x1 = [dt.datetime.strptime(d, "%Y-%m-%d").date() for d in data_y["Date"]]
y11= data_y["Adj Close"]
y1 = preprocessing.scale(data_y["Adj Close"])

plt.title("Precio de Cierre de GME de Enero 2020 a Enero 2022 ")
plt.xlabel("Dia")
plt.xticks(rotation=45)
plt.ylabel("Precio")
plt.plot_date(x1,y11,c="blue",ls="--" ,lw=2)
plt.show()

"""El gráfico de velas nos ayuda a describir el movimiento de precios de la compañía, el color de la vela representa el movimiento, el rojo es un precio de cierre mas bajo que el de apertura y una verde es un precio de cierre mas alto que el de apertura. La longuitud representa el rango de precios que se mueve en el día."""

import plotly.graph_objects as go

fig = go.Figure(data=[go.Candlestick(x=data_y['Date'],
                open=data_y['Open'],
                high=data_y['High'],
                low=data_y['Low'],
                close=data_y['Close'])])

fig.show()

data.info()

835033/1486198 # Postcon cuerpo

data.describe().transpose()

"""El usuario que aparece como "[deleted]" puede deberse a que el usuario borró el post, aunque el post permanece almacenado el registro del usuario permanece.
También se puede deber a que el usuario eliminó su cuenta, por lo que todos los comentarios y posts que realizó permanecen, pero no el nombre del usuario
"""

authors_count = data[['Title','Author']].groupby('Author').count() 
#Put them in order from highest to lowest
authors_count = authors_count.sort_values(by='Title', ascending=False)
authors_count.head(10)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
sub_aut = data['Author'].value_counts()
sub_aut = sub_aut[sub_aut >= 200]
sub_aut.plot(kind='pie', figsize=(10,10))

print(data.groupby(['Title']).size().sort_values())

print(data.groupby(['Publish Date']).size().sort_values())

data['Publish Date'] = data['Publish Date'].apply(lambda x: x.split(' ')[0])
#print(data.groupby(['Publish Date']).size().sort_values())
post_count = data[['Publish Date','Title']].groupby('Publish Date').count() 
post_count = post_count.sort_values(by='Title', ascending=False)
post_count.head(10)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
sub_date = data['Publish Date'].value_counts().sort_index()
sub_date.plot(kind='line', figsize=(10,10))

y=data['Publish Date'].unique()
x2 = [dt.datetime.strptime(d, "%Y-%m-%d").date() for d in y]
y22 = data['Publish Date'].value_counts().sort_index()
y2 = preprocessing.scale(data['Publish Date'].value_counts().sort_index())
pylab.rcParams['figure.figsize'] = (15, 9)   # Change the size of plots
plt.title("Volumen de noticias de wallstretbets de Enero 2020 a Enero 2022 ")
plt.xlabel("Dia")
plt.xticks(rotation=45)
plt.ylabel("Volumen")
plt.plot_date(x2,y22,c="blue",ls="--" ,lw=2)
plt.show()

def clean_text(text):
    text = str(text).lower()
    text = re.sub('\[.*?\]', '', text)
    text = re.sub('https?://\S+|www\.\S+', '', text)
    text = re.sub('<.*?>+', '', text)
    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub('\n', '', text)
    text = re.sub('\w*\d\w*', '', text)
    text = re.sub(r'\s+[a-zA-Z]\s+', '', text)
    doc = nlp(text)
    return ' '.join([token.text for token in doc if not token.is_stop]) #Elimina las stopwords

data['Title']

import spacy
nlp = spacy.blank('en')
title_data = data['Title'].apply(lambda x:clean_text(x))

title_data

data_title = title_data.apply(lambda x:str(x).split())
top = Counter([item for sublist in data_title for item in sublist])
temp = pd.DataFrame(top.most_common(20))
temp.columns = ['Common_words','count']
temp.style.background_gradient(cmap='Blues')

import plotly.express as px
fig = px.bar(temp, x="count", y="Common_words", title='Palabras comunes en el texto', orientation='h', 
             width=700, height=700,color='Common_words')
fig.show()

import spacy
nlp = spacy.blank('en')
#data['Title'].apply(lambda x:str(x).split())
title_data = data['Title'].apply(lambda x:clean_text(x))
data_proc = pd.concat([data['Publish Date'],title_data], axis=1)
data_proc['Title']  = data_proc.groupby(['Publish Date'])['Title'].transform(lambda x: ' '.join(x))
data_proc = data_proc.drop_duplicates()
data_proc

file_txt = (location+"data_procesada.csv")
data_proc.to_csv(file_txt)

data_proc = pd.read_csv(location +"data_procesada.csv")
del data_proc['Unnamed: 0']
count_unique_words = data_proc['Title'].apply(lambda x:len(set(str(x).split())))
df_unique_words = pd.concat([data_proc['Publish Date'],count_unique_words], axis=1)
df_unique_words

y = df_unique_words['Publish Date'].unique()
x3 = [dt.datetime.strptime(d, "%Y-%m-%d").date() for d in y]
y33 = df_unique_words['Title']
y3 = preprocessing.scale(df_unique_words['Title'])
pylab.rcParams['figure.figsize'] = (15, 9)   # Change the size of plots
plt.title("Palabras unicas en noticias de wallstretbets de Enero 2020 a Enero 2022 ")
plt.xlabel("Dia")
plt.xticks(rotation=45)
plt.ylabel("Palabras Unicas")
plt.plot_date(x3,y33,c="blue",ls="--" ,lw=2)
plt.show()

a = np.array(data['Publish Date'].value_counts().sort_index()) # Volumen de noticias por dia, n
b = np.array(df_unique_words['Title']) # Numero de palabras unicas por dia, p 
c = pd.DataFrame(np.divide(b,a),columns=['Razon']) # q= p/n
razon_q = pd.concat([data_proc['Publish Date'],c], axis=1)
razon_q # Si q<1 => PCA

y = razon_q['Publish Date'].unique()
x = [dt.datetime.strptime(d, "%Y-%m-%d").date() for d in y]
sub_date = razon_q['Razon']
pylab.rcParams['figure.figsize'] = (15, 9)   # Change the size of plots
plt.title("Razon en noticias y palabras de wallstretbets de Enero 2020 a Enero 2022 ")
plt.xlabel("Dia")
plt.xticks(rotation=45)
plt.ylabel("Razon")
plt.plot_date(x,sub_date,c="blue",ls="--" ,lw=2)
plt.show()

print("Dias que se puede realizar PCA: ",razon_q.apply(lambda x: x['Razon']<1, axis=1).sum())

pylab.rcParams['figure.figsize'] = (15, 9)   # Change the size of plots
plt.title("Noticias de wallstretbets de Enero 2020 a Enero 2022 ")
plt.xlabel("Dia")
plt.xticks(rotation=45)
plt.ylabel("Valor")
plt.plot_date(x1,y1, marker='+',c="red",ls="--" ,lw=1,label="Precio")
plt.plot_date(x2,y2, marker='x',c="green",ls="--" ,lw=1, label="Volumen")
plt.plot_date(x3,y3, marker='*',c="blue",ls="--" ,lw=1, label="Vocabulario")
plt.legend(loc="upper left")
plt.show()

# Juntamos toda la información es una sola tabla para calcular la coherencia
Precio = pd.DataFrame([x1,y11]).T
Precio.columns= ['Date','Precio']
Precio['Date']= Precio['Date'].unique()
Volumen = pd.DataFrame([x2,y22]).T
Volumen.columns= ['Date','Volumen']
Volumen['Date']= Volumen['Date'].unique()
Vocabulario = pd.DataFrame([x3,y33]).T
Vocabulario.columns= ['Date','Vocabulario']
Vocabulario['Date']= Vocabulario['Date'].unique()
temp = pd.merge(Precio,Volumen,on='Date')
Datos_puros = pd.merge(temp,Vocabulario,on='Date')
file_txt = (location+"data_conjunta_pura.csv")
Datos_puros.to_csv(file_txt)
Datos_puros

# Juntamos toda la información es una sola tabla para calcular la coherencia
Precio = pd.DataFrame([x1,y1]).T
Precio.columns= ['Date','Precio']
Precio['Date']= Precio['Date'].unique()
Volumen = pd.DataFrame([x2,y2]).T
Volumen.columns= ['Date','Volumen']
Volumen['Date']= Volumen['Date'].unique()
Vocabulario = pd.DataFrame([x3,y3]).T
Vocabulario.columns= ['Date','Vocabulario']
Vocabulario['Date']= Vocabulario['Date'].unique()
temp = pd.merge(Precio,Volumen,on='Date')
Datos = pd.merge(temp,Vocabulario,on='Date')
file_txt = (location+"data_conjunta.csv")
Datos.to_csv(file_txt)
Datos

data_est = pd.read_csv(location +"df_fin.csv")
del data_est['Unnamed: 0']
data_est

from statsmodels.tsa.stattools import grangercausalitytests
PV=grangercausalitytests(data_est[["Precio","Volumen"]],5)
VP=grangercausalitytests(data_est[["Volumen","Precio"]],5)

PVo=grangercausalitytests(data_est[["Precio","Vocabulario"]],5)
VoP=grangercausalitytests(data_est[["Vocabulario","Precio"]],5)

VVo=grangercausalitytests(data_est[["Volumen","Vocabulario"]],5)
VoV=grangercausalitytests(data_est[["Vocabulario","Volumen"]],5)

import statsmodels.api as sm
from statsmodels.tsa.stattools import grangercausalitytests
import numpy as np
data = sm.datasets.macrodata.load_pandas()
data = data.data[["realgdp", "realcons"]].pct_change().dropna()
gc_res = grangercausalitytests(data,2)
#gc_res = grangercausalitytests(data,)
gc_res

serie_precio = data_y[["Date","Close"]]

#sub_date = data['Publish Date'].value_counts().sort_index()
#sub_date = pd.DataFrame(sub_date)
#sub_date

sub_date = data['Publish Date'].value_counts().sort_index()
sub_date.reset_index(drop=True, inplace=True)
serie_vol = pd.concat([razon_q['Publish Date'],sub_date],axis=1)

serie_uni = pd.concat([razon_q['Publish Date'],pd.DataFrame(b,columns=["W_unique"])],axis=1)

file_txt = (location+"serie_precio.csv")
serie_precio.to_csv(file_txt)

file_txt = (location+"serie_volumen.csv")
serie_vol.to_csv(file_txt)

file_txt = (location+"serie_unica.csv")
serie_uni.to_csv(file_txt)

# Commented out IPython magic to ensure Python compatibility.
#!apt-get install r-base
#!pip install rpy2
# %load_ext rpy2.ipython

type(Datos)

# Commented out IPython magic to ensure Python compatibility.
# %R -i sub_date
# %R -i serie_precio

# Commented out IPython magic to ensure Python compatibility.
# %%R
# install.packages('biwavelet')
# library(biwavelet) 
#

# Commented out IPython magic to ensure Python compatibility.
# %%R 
# A = as.data.frame(cbind(serie_precio$Date,serie_precio$Close))
# B = as.data.frame(cbind(serie_precio$Date,sub_date$'Publish Date'))
# length(A)
# A
# #wtc(A,B,nrands=200)

serie_precio

from collections import Counter
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
data_title = data['Title'].apply(lambda x:str(x).split())
stop_words = set(stopwords.words('english'))
stop_words.add('-')
data_title = [w for w in data_title if not w in stop_words]
top = Counter([item for sublist in data_title for item in sublist])
temp = pd.DataFrame(top.most_common(20))
temp.columns = ['Common_words','count']
temp.style.background_gradient(cmap='Blues')

import plotly.express as px
fig = px.bar(temp, x="count", y="Common_words", title='Palabras comunes en el texto', orientation='h', 
             width=700, height=700,color='Common_words')
fig.show()

# Commented out IPython magic to ensure Python compatibility.
import numpy as np 
import pandas as pd
import matplotlib
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline 
from wordcloud import WordCloud, STOPWORDS
from nltk.sentiment import SentimentIntensityAnalyzer
from textblob import TextBlob
import warnings
warnings.simplefilter("ignore")

def show_wordcloud(data, title=""):
    text = " ".join(t for t in data.dropna())
    stopwords = set(STOPWORDS)
    stopwords.update(["t", "co", "https", "amp", "U","DD"])
    wordcloud = WordCloud(stopwords=stopwords, scale=4, max_font_size=50, max_words=500,background_color="black").generate(text)
    fig = plt.figure(1, figsize=(16,16))
    plt.axis('off')
    fig.suptitle(title, fontsize=20)
    fig.subplots_adjust(top=2.3)
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.show()

show_wordcloud(data['Title'], title = 'Palabras principales en el titulo')

import nltk
nltk.download('vader_lexicon')

sia = SentimentIntensityAnalyzer()

sia.lexicon

sia = SentimentIntensityAnalyzer()
def find_sentiment(post):
    if sia.polarity_scores(post)["compound"] > 0:
        return "Positive"
    elif sia.polarity_scores(post)["compound"] < 0:
        return "Negative"
    else:
        return "Neutral"

def plot_sentiment(df, feature, title):
    counts = df[feature].value_counts()
    percent = counts/sum(counts)

    fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))

    counts.plot(kind='bar', ax=ax1, color='green')
    percent.plot(kind='bar', ax=ax2, color='blue')
    ax1.set_ylabel(f'Contador : {title} sentimientos', size=12)
    ax2.set_ylabel(f'Porcentaje : {title} sentimientos', size=12)
    plt.suptitle(f"Analisis de sentimiento: {title}")
    plt.tight_layout()
    plt.show()

data_proc

data_proc['title_sentiment'] = data_proc['Title'].apply(lambda x: find_sentiment(x))
plot_sentiment(data_proc, 'title_sentiment', 'Titulo')

show_wordcloud(data_proc.loc[data_proc['title_sentiment']=='Positive', 'Title'], title = 'Palabras principales en titulos (Sentimiento positivo)')

show_wordcloud(data_proc.loc[data_proc['title_sentiment']=='Negative', 'Title'], title = 'Palabras principales en titulos (Sentimiento negativo)')

#pip install datasets

#pip install umap-learn

#pip install hdbscan

#pip install bertopic

#pip install distributed==2021.9.1



"""## BERTopic

"""

from datasets import load_dataset

data = load_dataset(
    'jamescalam/reddit-topics', split='train',
    revision='c14d532'  # this specifies current version of dataset
)
# remove short self text
data = data.filter(lambda x: True if len(x['selftext']) > 30 else 0)
data

from umap import UMAP
from hdbscan import HDBSCAN

umap_model = UMAP(n_neighbors=3, n_components=3, min_dist=0.05)
hdbscan_model = HDBSCAN(min_cluster_size=80, min_samples=40,
                        prediction_data=True, gen_min_span_tree=True)

type(data)

data_proc['Title']

type(data_proc)

from bertopic import BERTopic

help(BERTopic)

from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import CountVectorizer
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords


stopwords = list(stopwords.words('english')) + ['http', 'https', 'amp', 'com']

embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
# we add this to remove stopwords that can pollute topcs
vectorizer_model = CountVectorizer(ngram_range=(1, 1), stop_words=stopwords)

model = BERTopic(
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    embedding_model=embedding_model,
    vectorizer_model=vectorizer_model,
    top_n_words=2,
    language='english',
    calculate_probabilities=True,
    verbose=True
)
#model.fit(data['selftext'])
topics, probs = model.fit_transform(data_proc['Title'])

topics2, probs2 = model.fit_transform(title_data)

i =450
print(f"{topics2[i]}: {data_proc['Title'][i]}")

for i in range(5):
    print(f"{topics[i]}: {data_proc['Title'][i]}")

freq = model.get_topic_info()
freq.head(10)

model.visualize_topics()

model.visualize_barchart()

model.visualize_hierarchy()
