# -*- coding: utf-8 -*-
"""BERTopic_Reddit.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rSwxwHVORoCizhjYtw_GUwBiElT28QqZ
"""

location = "/home/josegomez/"

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd # Uso de dataframes
import csv # Para convertir las tablas finales en archivos csv y guardarla en la m치quina local
import datetime # Manipulaci칩n del tiempo
import numpy as np 
import matplotlib
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import datetime as dt
# %matplotlib inline 
from wordcloud import WordCloud, STOPWORDS
from textblob import TextBlob
from collections import Counter
import re
import string
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from nltk.sentiment.vader import SentimentIntensityAnalyzer as vad
from sklearn import preprocessing


from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import CountVectorizer
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords


from umap import UMAP
from hdbscan import HDBSCAN

#data_ws = pd.read_csv(location +"data/WallStreetbets.csv")
#data_y = pd.read_csv(location +"data/Finance/Yahoo_GME.csv")
#data_proc = pd.read_csv(location +"data/data_procesada.csv")
data_titles = pd.read_csv(location +"data/data_title.csv")

#del data_ws['Unnamed: 0']
#del data_proc['Unnamed: 0']
del data_titles['Unnamed: 0']

#data_ws['Publish Date'] = data_ws['Publish Date'].apply(lambda x: x.split(' ')[0])

def remove_emoji(string):
    emoji_pattern = re.compile("["
                               u"\U0001F600-\U0001F64F"  # emoticons
                               u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                               u"\U0001F680-\U0001F6FF"  # transport & map symbols
                               u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                               u"\U00002500-\U00002BEF"  # chinese char
                               u"\U00002702-\U000027B0"
                               u"\U00002702-\U000027B0"
                               u"\U000024C2-\U0001F251"
                               u"\U0001f926-\U0001f937"
                               u"\U00010000-\U0010ffff"
                               u"\u2640-\u2642"
                               u"\u2600-\u2B55"
                               u"\u200d"
                               u"\u23cf"
                               u"\u23e9"
                               u"\u231a"
                               u"\ufe0f"  # dingbats
                               u"\u3030"
                               "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', string)

data_titles['Title'] = data_titles['Title'].apply(str)
data_titles['Title'] = data_titles['Title'].apply(lambda x:remove_emoji(x))

# Se eliminan los registros que tengan menos de 20 y 10 caracteres para hacer comparaci칩n
data_titles["Long"] = data_titles["Title"].str.len()
data_title_filtered = data_titles[data_titles["Long"]>= 40]
data_title_filtered2 = data_titles[data_titles["Long"]>= 10]

# Modelo UMAP y HDBSCAN
# min_cluster_size=80, min_samples=40,
umap_model = UMAP(n_neighbors=10, n_components=3, min_dist=0.1)
hdbscan_model = HDBSCAN(min_cluster_size=80, min_samples=40,
                        prediction_data=True, gen_min_span_tree=True)


stopwords = list(stopwords.words('english')) + ['http', 'https', 'amp', 'com']

embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
# metodo para eliminar las stopwords
vectorizer_model = CountVectorizer(ngram_range=(1, 1), stop_words=stopwords)

# Definici칩n del modelo
model = BERTopic(
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    embedding_model=embedding_model,
    vectorizer_model=vectorizer_model,
    min_topic_size = 20,
    top_n_words=10,
    language='english',
    calculate_probabilities=True,
    verbose=True
)

# Ajuste del modelo en los datos
topics, probs = model.fit_transform(data_title_filtered['Title'].tolist())

#for i in range(5):
#    print(f"{topics[i]}: {data_proc['Title'][i]}")

freq = model.get_topic_info()
freq.head(10)

#model.visualize_topics()

#model.visualize_barchart()

#model.visualize_hierarchy()

