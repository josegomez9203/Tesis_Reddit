# -*- coding: utf-8 -*-
"""BERTopic_Reddit.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rSwxwHVORoCizhjYtw_GUwBiElT28QqZ
"""

location = "/home/josegomez/"

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd # Uso de dataframes
import csv # Para convertir las tablas finales en archivos csv y guardarla en la m치quina local
import datetime # Manipulaci칩n del tiempo
import numpy as np 
import matplotlib
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import datetime as dt
# %matplotlib inline 
from wordcloud import WordCloud, STOPWORDS
from textblob import TextBlob
from collections import Counter
import re
import string
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from nltk.sentiment.vader import SentimentIntensityAnalyzer as vad
from sklearn import preprocessing


from bertopic import BERTopic
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import CountVectorizer
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords


from umap import UMAP
from hdbscan import HDBSCAN

#data_ws = pd.read_csv(location +"data/WallStreetbets.csv")
#data_y = pd.read_csv(location +"data/Finance/Yahoo_GME.csv")
#data_proc = pd.read_csv(location +"data/data_procesada.csv")
data_title = pd.read_csv(location +"data/data_title.csv")

#del data_ws['Unnamed: 0']
#del data_proc['Unnamed: 0']
del data_title['Unnamed: 0']

#data_ws['Publish Date'] = data_ws['Publish Date'].apply(lambda x: x.split(' ')[0])

# Se eliminan los registros que tengan menos de 20 y 10 caracteres para hacer comparaci칩n
data_title["Long"] = data_title["Title"].str.len()
data_title_filtered = data_title[data_title["Long"]>= 20]
data_title_filtered2 = data_title[data_title["Long"]>= 10]

# Modelo UMAP y HDBSCAN
# min_cluster_size=80, min_samples=40,
umap_model = UMAP(n_neighbors=10, n_components=3, min_dist=0.1)
hdbscan_model = HDBSCAN(prediction_data=True, gen_min_span_tree=True)


stopwords = list(stopwords.words('english')) + ['http', 'https', 'amp', 'com']

embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
# metodo para eliminar las stopwords
vectorizer_model = CountVectorizer(ngram_range=(1, 1), stop_words=stopwords)

# Definici칩n del modelo
model = BERTopic(
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    embedding_model=embedding_model,
    vectorizer_model=vectorizer_model,
    top_n_words=2,
    language='english',
    calculate_probabilities=True,
    verbose=True
)

# Ajuste del modelo en los datos
topics, probs = model.fit_transform(data_title_filtered['Title'].tolist())

#for i in range(5):
#    print(f"{topics[i]}: {data_proc['Title'][i]}")

freq = model.get_topic_info()
freq.head(10)

#model.visualize_topics()

#model.visualize_barchart()

#model.visualize_hierarchy()

